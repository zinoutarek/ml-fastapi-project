# Dockerfile for GPU-based ML inference with CUDA support
# Uncomment and use for GPU deployment

# Use NVIDIA CUDA base image
FROM nvidia/cuda:13.1.1-runtime-ubuntu22.04

ENV PYTHONUNBUFFERED=1

# Install Python
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install uv
COPY --from=ghcr.io/astral-sh/uv:0.9.26 /uv /uvx /bin/

ENV UV_COMPILE_BYTECODE=1
ENV UV_LINK_MODE=copy

WORKDIR /app/

ENV PATH="/app/.venv/bin:$PATH"

# Install CUDA-enabled ONNX Runtime
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --frozen --no-install-workspace --package app

COPY ./backend/scripts /app/backend/scripts
COPY ./backend/pyproject.toml ./backend/alembic.ini /app/backend/
COPY ./backend/app /app/backend/app

RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --frozen --package app

# Install CUDA version of ONNX Runtime
RUN pip install --upgrade onnxruntime-gpu

# Pre-train ML model
ENV PRETRAIN_MODEL=1
RUN if [ "$PRETRAIN_MODEL" = "1" ]; then \
    python -c "from app.ml.model import get_model; get_model()" ; \
    fi

WORKDIR /app/backend/

# Use CUDA provider for inference
ENV ONNXRUNTIME_EXECUTION_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider

CMD ["fastapi", "run", "--workers", "4", "app/main.py"]
